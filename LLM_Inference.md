LLM Inference - token generation using decoder only transformer models since most challenges and their associated remediation come from that particular architecture and use case.

Token generation using a transformer decoder consists of two kinds of steps, the prompt processing step and multiple autoregressive steps, with very different hardware utilization profiles. 


