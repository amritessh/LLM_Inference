quantization and model optimization:

Precision reduction for performance
understanding number representation
FP32 baseline Precision

quantization performance impact

Memory and speed benefits

Model Pruning, removing redundant parameters

unstructured pruning, remove individual weights

structured pruning, remove entire channels/neurons

magnitude based pruning 

gradient based pruning 

Fisher information pruning: 

. Model Formats: Optimized Inference Engines
ONNX (Open Neural Network Exchange)


